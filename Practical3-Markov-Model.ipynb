{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lWZ3pFW388H",
        "outputId": "4d815cd4-0190-4a18-b7d2-b4105f6211cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-1084b1ff9736>:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'None' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  data.fillna(\"None\", inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5268 - loss: 1.4585\n",
            "Epoch 2/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6328 - loss: 0.9585\n",
            "Epoch 3/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4046 - loss: 0.9426\n",
            "Epoch 4/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5641 - loss: 0.9722\n",
            "Epoch 5/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6209 - loss: 0.8799\n",
            "Epoch 6/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5867 - loss: 0.9474\n",
            "Epoch 7/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5876 - loss: 0.9485\n",
            "Epoch 8/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6079 - loss: 0.9261\n",
            "Epoch 9/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5978 - loss: 0.9024\n",
            "Epoch 10/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5911 - loss: 0.9652\n",
            "Epoch 11/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5554 - loss: 0.9618\n",
            "Epoch 12/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5425 - loss: 0.9444\n",
            "Epoch 13/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5834 - loss: 0.8880\n",
            "Epoch 14/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5752 - loss: 0.9080\n",
            "Epoch 15/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5584 - loss: 0.9418\n",
            "Epoch 16/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5610 - loss: 0.9572\n",
            "Epoch 17/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5937 - loss: 0.9567\n",
            "Epoch 18/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5855 - loss: 0.9268\n",
            "Epoch 19/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6106 - loss: 0.8924\n",
            "Epoch 20/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6298 - loss: 0.8917\n",
            "Epoch 21/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6161 - loss: 0.8715\n",
            "Epoch 22/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5967 - loss: 0.8899\n",
            "Epoch 23/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5890 - loss: 0.8942\n",
            "Epoch 24/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6090 - loss: 0.9583\n",
            "Epoch 25/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5968 - loss: 0.9381\n",
            "Epoch 26/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5782 - loss: 0.9556\n",
            "Epoch 27/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6037 - loss: 0.9095\n",
            "Epoch 28/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6359 - loss: 0.8717\n",
            "Epoch 29/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6148 - loss: 0.8990\n",
            "Epoch 30/30\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5873 - loss: 0.9189\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
            "Next predicted purchase: 1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Markov_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Fill NaN values with a placeholder to indicate no purchase\n",
        "data.fillna(\"None\", inplace=True)\n",
        "\n",
        "# Reshape data to track transitions by customer\n",
        "# Here, we group the purchases of each customer across months into sequences\n",
        "customer_data = data.groupby('Customer').apply(lambda x: x.drop('Customer', axis=1).values.flatten())\n",
        "sequences = customer_data.tolist()\n",
        "\n",
        "# Flatten sequences and encode the product categories\n",
        "flattened_sequences = [item for sublist in sequences for item in sublist]\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(flattened_sequences)\n",
        "\n",
        "# Reshape the encoded data back into the original sequence format\n",
        "sequences_encoded = [integer_encoded[i:i + len(seq)] for i, seq in enumerate(sequences)]\n",
        "\n",
        "# Define the maximum sequence length\n",
        "max_sequence_len = max([len(seq) for seq in sequences_encoded])\n",
        "\n",
        "# Pad sequences so that they all have the same length\n",
        "padded_sequences = pad_sequences(sequences_encoded, maxlen=max_sequence_len, padding='post')\n",
        "\n",
        "# Split the sequences into input (X) and target (y)\n",
        "X, y = [], []\n",
        "for seq in padded_sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        X.append(seq[:i])  # Input: sequence up to this point\n",
        "        y.append(seq[i])   # Target: next item in the sequence\n",
        "\n",
        "# Pad sequences to ensure all inputs are the same length\n",
        "X = pad_sequences(X, maxlen=max_sequence_len, padding='post')\n",
        "\n",
        "# One-hot encode the target values\n",
        "y = to_categorical(y, num_classes=len(label_encoder.classes_))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(label_encoder.classes_), 10, input_length=max_sequence_len))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=30, batch_size=32)\n",
        "\n",
        "# Prediction example\n",
        "def predict_next_purchase(sequence):\n",
        "    encoded_sequence = label_encoder.transform(sequence)\n",
        "    padded_input = pad_sequences([encoded_sequence], maxlen=max_sequence_len, padding='post')\n",
        "    prediction = model.predict(padded_input)\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    return label_encoder.inverse_transform([predicted_index])[0]\n",
        "\n",
        "# Example prediction for a sequence starting with \"Shoes\"\n",
        "example_sequence = ['Shoes']\n",
        "next_purchase = predict_next_purchase(example_sequence)\n",
        "print(f'Next predicted purchase: {next_purchase}')\n"
      ]
    }
  ]
}